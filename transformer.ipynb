{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torchvision.transforms as transforms\n",
    "    from torchvision.models import vit_b_16\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from torch.optim import AdamW\n",
    "    from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "    # Load and Preprocess the FER2013 Dataset\n",
    "    class FER2013Dataset(Dataset):\n",
    "        def __init__(self, data, transform=None):\n",
    "            self.data = data\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            pixels = np.fromstring(self.data.iloc[idx]['pixels'], dtype=int, sep=' ')\n",
    "            image = pixels.reshape(48, 48).astype(np.uint8)\n",
    "            image = Image.fromarray(image)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            label = int(self.data.iloc[idx]['emotion'])\n",
    "            return image, label\n",
    "\n",
    "    # Define transformation to increase input resolution and apply data augmentation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),  # Grayscale to RGB\n",
    "        transforms.Resize((224, 224)),                  # Increase input resolution\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))          # Normalization\n",
    "    ])\n",
    "\n",
    "    # Load FER2013 data\n",
    "    data = pd.read_csv(\"C:/Users/dang0/Downloads/fer2013.csv\")\n",
    "    train_data = FER2013Dataset(data[data['Usage'] == 'Training'], transform=transform)\n",
    "    val_data = FER2013Dataset(data[data['Usage'] == 'PublicTest'], transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Initialize Pre-trained Vision Transformer Model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = vit_b_16(pretrained=True) \n",
    "    num_classes = 7  # FER2013 has 7 emotion classes\n",
    "    model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Freeze initial layers for transfer learning\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.heads.head.parameters():\n",
    "        param.requires_grad = True  # Only train the final layer at first\n",
    "\n",
    "    # Define Focal Loss\n",
    "    def focal_loss(inputs, targets, alpha=0.25, gamma=2):\n",
    "        bce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = alpha * (1 - pt) ** gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "    # Optimizer and Scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    # Training Loop with Focal Loss\n",
    "    num_epochs = 20\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = focal_loss(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Validation loop for accuracy measurement\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "        # Save the model if accuracy improves\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), 'best_vit_emotion_model.pth')\n",
    "\n",
    "        # Gradually unfreeze layers if accuracy improves\n",
    "        if epoch % 5 == 0 and accuracy > best_accuracy * 0.9:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True  # Unfreeze more layers gradually\n",
    "\n",
    "    print(f\"Training complete. Best validation accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Initialize lists to store loss and accuracy for plotting\n",
    "# train_losses = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# # Training Loop with Focal Loss\n",
    "# num_epochs = 20\n",
    "# best_accuracy = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "#         loss = focal_loss(outputs, labels)\n",
    "\n",
    "#         # Backward and optimize\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     scheduler.step()  # Update learning rate\n",
    "\n",
    "#     # Validation loop for accuracy measurement\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in val_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     accuracy = 100 * correct / total\n",
    "#     train_losses.append(running_loss / len(train_loader))\n",
    "#     val_accuracies.append(accuracy)\n",
    "\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "#     # Save the model if accuracy improves\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         torch.save(model.state_dict(), 'best_vit_emotion_model.pth')\n",
    "\n",
    "#     # Gradually unfreeze layers if accuracy improves\n",
    "#     if epoch % 5 == 0 and accuracy > best_accuracy * 0.9:\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = True  # Unfreeze more layers gradually\n",
    "\n",
    "# print(f\"Training complete. Best validation accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "# # Plot training loss and validation accuracy\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# # Plotting training loss\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss Over Epochs')\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "\n",
    "# # Plotting validation accuracy\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='green')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.title('Validation Accuracy Over Epochs')\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# OpenCV setup for real-time emotion detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (48, 48))\n",
    "    img_tensor = transform(Image.fromarray(resized)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Get prediction\\\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        emotion = emotion_labels[predicted.item()]\n",
    "\n",
    "    # Display emotion on frame\n",
    "    cv2.putText(frame, emotion, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('Emotion Recognition', frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
